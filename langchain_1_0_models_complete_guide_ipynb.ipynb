{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/masx200/langchain_1_0_models_complete_guide_ipynb/blob/main/langchain_1_0_models_complete_guide_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOpHFZO7wDWV"
      },
      "source": [
        "# LangChain 1.0 模型声明与使用完全指南\n",
        "\n",
        "本notebook全面介绍LangChain 1.0中模型的声明和使用方法，突出其灵活性和多样性。\n",
        "\n",
        "## 目录\n",
        "1. [环境准备](#1-环境准备)\n",
        "2. [基础：固定模型声明](#2-基础固定模型声明)\n",
        "3. [灵活：无默认模型的可配置模型](#3-灵活无默认模型的可配置模型)\n",
        "4. [混合：有默认模型的可配置模型](#4-混合有默认模型的可配置模型)\n",
        "5. [高级：绑定工具的可配置模型](#5-高级绑定工具的可配置模型)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF8HFWWIwDWW"
      },
      "source": [
        "## 1. 环境准备\n",
        "\n",
        "安装必要的依赖包："
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-openai langchain-anthropic langchain-google-vertexai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErKgK3PywUmY",
        "outputId": "8b8ec0d4-e7b0-429b-d5d9-0794891d5aef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (0.3.35)\n",
            "Requirement already satisfied: langchain-anthropic in /usr/local/lib/python3.12/dist-packages (0.3.22)\n",
            "Requirement already satisfied: langchain-google-vertexai in /usr/local/lib/python3.12/dist-packages (2.1.2)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.79)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.41)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.44)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.3)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.104.2 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.109.1)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: anthropic<1.0.0,>=0.69.0 in /usr/local/lib/python3.12/dist-packages (from langchain-anthropic) (0.72.1)\n",
            "Requirement already satisfied: google-cloud-aiplatform>=1.97.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-vertexai) (1.125.0)\n",
            "Requirement already satisfied: google-cloud-storage<3,>=2.18 in /usr/local/lib/python3.12/dist-packages (from langchain-google-vertexai) (2.19.0)\n",
            "Requirement already satisfied: httpx<1,>=0.28 in /usr/local/lib/python3.12/dist-packages (from langchain-google-vertexai) (0.28.1)\n",
            "Requirement already satisfied: httpx-sse<1,>=0.4 in /usr/local/lib/python3.12/dist-packages (from langchain-google-vertexai) (0.4.3)\n",
            "Requirement already satisfied: validators<1,>=0.22 in /usr/local/lib/python3.12/dist-packages (from langchain-google-vertexai) (0.35.0)\n",
            "Requirement already satisfied: bottleneck<2,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain-google-vertexai) (1.4.2)\n",
            "Requirement already satisfied: numexpr<3,>=2.8.6 in /usr/local/lib/python3.12/dist-packages (from langchain-google-vertexai) (2.14.1)\n",
            "Requirement already satisfied: pyarrow<22,>=19.0.1 in /usr/local/lib/python3.12/dist-packages (from langchain-google-vertexai) (21.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from anthropic<1.0.0,>=0.69.0->langchain-anthropic) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from anthropic<1.0.0,>=0.69.0->langchain-anthropic) (1.9.0)\n",
            "Requirement already satisfied: docstring-parser<1,>=0.15 in /usr/local/lib/python3.12/dist-packages (from anthropic<1.0.0,>=0.69.0->langchain-anthropic) (0.17.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from anthropic<1.0.0,>=0.69.0->langchain-anthropic) (0.11.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from anthropic<1.0.0,>=0.69.0->langchain-anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from anthropic<1.0.0,>=0.69.0->langchain-anthropic) (4.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from bottleneck<2,>=1.4->langchain-google-vertexai) (2.0.2)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform>=1.97.0->langchain-google-vertexai) (2.28.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform>=1.97.0->langchain-google-vertexai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform>=1.97.0->langchain-google-vertexai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform>=1.97.0->langchain-google-vertexai) (5.29.5)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform>=1.97.0->langchain-google-vertexai) (25.0)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform>=1.97.0->langchain-google-vertexai) (3.38.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0,>=1.3.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform>=1.97.0->langchain-google-vertexai) (1.15.0)\n",
            "Requirement already satisfied: shapely<3.0.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform>=1.97.0->langchain-google-vertexai) (2.1.2)\n",
            "Requirement already satisfied: google-genai<2.0.0,>=1.37.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform>=1.97.0->langchain-google-vertexai) (1.49.0)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage<3,>=2.18->langchain-google-vertexai) (2.5.0)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage<3,>=2.18->langchain-google-vertexai) (2.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage<3,>=2.18->langchain-google-vertexai) (1.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.28->langchain-google-vertexai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.28->langchain-google-vertexai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.28->langchain-google-vertexai) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.28->langchain-google-vertexai) (0.16.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform>=1.97.0->langchain-google-vertexai) (1.71.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform>=1.97.0->langchain-google-vertexai) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform>=1.97.0->langchain-google-vertexai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform>=1.97.0->langchain-google-vertexai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform>=1.97.0->langchain-google-vertexai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform>=1.97.0->langchain-google-vertexai) (4.9.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform>=1.97.0->langchain-google-vertexai) (2.9.0.post0)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0,>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-resource-manager<3.0.0,>=1.3.3->google-cloud-aiplatform>=1.97.0->langchain-google-vertexai) (0.14.3)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform>=1.97.0->langchain-google-vertexai) (15.0.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform>=1.97.0->langchain-google-vertexai) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.8.2->google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform>=1.97.0->langchain-google-vertexai) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlq4dGd1wDWW"
      },
      "outputs": [],
      "source": [
        "# 安装核心包和提供商集成\n",
        "# !pip install langchain langchain-openai langchain-anthropic langchain-google-vertexai\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# 设置API密钥\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# 如果使用Anthropic\n",
        "# if \"ANTHROPIC_API_KEY\" not in os.environ:\n",
        "#     os.environ[\"ANTHROPIC_API_KEY\"] = getpass(\"请输入Anthropic API Key: \")\n",
        "\n",
        "# 如果使用Google Vertex AI\n",
        "# if \"GOOGLE_APPLICATION_CREDENTIALS\" not in os.environ:\n",
        "#     os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"path/to/credentials.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMDdTGVJwDWW"
      },
      "source": [
        "## 2. 基础：固定模型声明\n",
        "\n",
        "最直接的使用方式：声明固定的模型并调用。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QXERTcRwDWW"
      },
      "source": [
        "### 2.1 声明不同提供商的模型\n",
        "\n",
        "使用 `provider:model` 格式明确指定提供商和模型。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJ5wqx23wDWW",
        "outputId": "2728df31-3f13-4b34-a244-83df77701723"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "模型声明完成\n"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# 声明OpenAI模型\n",
        "o3_mini = init_chat_model(\"openai:o3-mini\")\n",
        "\n",
        "# 声明Anthropic模型\n",
        "claude_sonnet = init_chat_model(\"anthropic:claude-sonnet-4-5-20250929\", temperature=0)\n",
        "\n",
        "# 声明Google Vertex AI模型\n",
        "# gemini_2_5_flash = init_chat_model(\"google_vertexai:gemini-2.5-flash\", temperature=0)\n",
        "\n",
        "print(\"模型声明完成\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(o3_mini.dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BrSlULjwvSv",
        "outputId": "df62994d-ae2b-40ec-b3dd-86cfdf6e7cc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model_name': 'o3-mini', 'model': 'o3-mini', 'stream': False, '_type': 'openai-chat'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gi2HHZSlwDWW"
      },
      "source": [
        "### 2.2 调用不同模型\n",
        "\n",
        "每个模型独立调用，返回各自的结果。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "1wy2I2U8wDWW",
        "outputId": "095f3961-db1a-4bef-9e21-903680ba531d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O3-mini: I'm ChatGPT—a language model created by OpenAI. How can I help you today?\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "\"Could not resolve authentication method. Expected either api_key or auth_token to be set. Or for one of the `X-Api-Key` or `Authorization` headers to be explicitly omitted\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2693767236.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 调用Anthropic模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mresponse_claude\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclaude_sonnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"what's your name\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nClaude Sonnet:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_claude\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m         return cast(\n\u001b[1;32m    394\u001b[0m             \u001b[0;34m\"ChatGeneration\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    396\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     ) -> LLMResult:\n\u001b[1;32m   1024\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1025\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m                 results.append(\n\u001b[0;32m--> 842\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    843\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1089\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m   1092\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_anthropic/chat_models.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1762\u001b[0m         \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_request_payload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1763\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1764\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1765\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0manthropic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBadRequestError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1766\u001b[0m             \u001b[0m_handle_anthropic_bad_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_anthropic/chat_models.py\u001b[0m in \u001b[0;36m_create\u001b[0;34m(self, payload)\u001b[0m\n\u001b[1;32m   1621\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"betas\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpayload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1623\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1625\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_acreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpayload\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/anthropic/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/anthropic/resources/messages/messages.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, max_tokens, messages, model, metadata, service_tier, stop_sequences, stream, system, temperature, thinking, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    925\u001b[0m             )\n\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0;34m\"/v1/messages\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1324\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m         )\n\u001b[0;32m-> 1326\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m             \u001b[0mremaining_retries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_retries\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mretries_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m             \u001b[0mrequest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretries_taken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretries_taken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36m_build_request\u001b[0;34m(self, options, retries_taken)\u001b[0m\n\u001b[1;32m    504\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unexpected JSON data type, {type(json_data)}, cannot merge with `extra_body`\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_headers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretries_taken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretries_taken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_merge_mappings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0mcontent_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Content-Type\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36m_build_headers\u001b[0;34m(self, options, retries_taken)\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0mcustom_headers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         )\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_headers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheaders_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_headers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0;31m# headers are case-insensitive while dictionaries are not.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/anthropic/_client.py\u001b[0m in \u001b[0;36m_validate_headers\u001b[0;34m(self, headers, custom_headers)\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         raise TypeError(\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;34m'\"Could not resolve authentication method. Expected either api_key or auth_token to be set. Or for one of the `X-Api-Key` or `Authorization` headers to be explicitly omitted\"'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         )\n",
            "\u001b[0;31mTypeError\u001b[0m: \"Could not resolve authentication method. Expected either api_key or auth_token to be set. Or for one of the `X-Api-Key` or `Authorization` headers to be explicitly omitted\""
          ]
        }
      ],
      "source": [
        "# 调用OpenAI模型\n",
        "response_o3 = o3_mini.invoke(\"what's your name\")\n",
        "print(\"O3-mini:\", response_o3.content)\n",
        "\n",
        "# 调用Anthropic模型\n",
        "response_claude = claude_sonnet.invoke(\"what's your name\")\n",
        "print(\"\\nClaude Sonnet:\", response_claude.content)\n",
        "\n",
        "# 调用Google模型\n",
        "# response_gemini = gemini_2_5_flash.invoke(\"what's your name\")\n",
        "# print(\"\\nGemini 2.5 Flash:\", response_gemini.content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "o3_mini_model = ChatOpenAI(model=\"o3-mini\")"
      ],
      "metadata": {
        "id": "n4wlwAIW5m2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "o3_mini_model.invoke(\"Hi\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NujcJeOb5tWi",
        "outputId": "73178738-b3ac-4761-eac9-9ebbe4171c46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello! How can I help you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 150, 'prompt_tokens': 7, 'total_tokens': 157, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 128, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'o3-mini-2025-01-31', 'system_fingerprint': 'fp_d83b50479d', 'id': 'chatcmpl-CaqZe4O8JFI0hZiuWxysTvoxHfCJi', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--51865903-63ab-4e26-9e4f-fb8bf5452549-0', usage_metadata={'input_tokens': 7, 'output_tokens': 150, 'total_tokens': 157, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 128}})"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAlTHJ3LwDWW"
      },
      "source": [
        "### 2.3 自动推断提供商\n",
        "\n",
        "LangChain可以根据模型名称前缀自动推断提供商。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpKLj-yJwDWW",
        "outputId": "113f7a79-6b84-4172-83a4-d4803c566a91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT-4o-mini: I’m called ChatGPT. How can I assist you today?\n"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# 不需要显式指定提供商，系统自动识别\n",
        "gpt_4o = init_chat_model(\"gpt-4o-mini\", temperature=0)  # 自动识别为OpenAI\n",
        "# claude = init_chat_model(\"claude-sonnet-4-5-20250929\", temperature=0)  # 自动识别为Anthropic\n",
        "\n",
        "response = gpt_4o.invoke(\"what's your name\")\n",
        "print(\"GPT-4o-mini:\", response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvluJjfywDWW"
      },
      "source": [
        "### 2.4 设置模型参数\n",
        "\n",
        "在初始化时设置temperature、max_tokens等参数。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Srah8ttSwDWX",
        "outputId": "6a535ec0-9665-4c0d-fd13-2d966dd2ffcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temperature=0:\n",
            "当然可以！这是一个简单的笑话：\n",
            "\n",
            "有一天，一只鸡走进了图书馆。它走到图书馆员那里，咕咕叫了几声。图书馆员有些困惑，但还是给了它一本书。鸡拿着书走了出去。\n",
            "\n",
            "第二天，鸡又来了，还是咕咕叫。图书馆员又给了它一本书。鸡又走了。\n",
            "\n",
            "第三天，鸡再次来到图书馆，这次它叫得更急了。图书馆员决定跟着它，看看它到底在做什么。\n",
            "\n",
            "鸡走到一个池塘边，把书递给了一只青蛙。青蛙看了看书，摇了摇头，说：“呱，不是这个，呱，不是这个！”\n",
            "\n",
            "希望这个笑话能让你笑一笑！\n",
            "\n",
            "Temperature=1:\n",
            "当然可以！这里有一个笑话：\n",
            "\n",
            "为什么海洋总是那么蓝？\n",
            "\n",
            "因为鱼在游的时候总是“藍藍藍”！ \n",
            "\n",
            "希望这个笑话能让你开心！\n",
            "\n",
            "Max tokens=50:\n",
            "当然可以！这是一个简单的笑话：\n",
            "\n",
            "有一天，一只鸡走进了图书馆。它走到图书馆员那里，咕咕地说：“书，书，书！”  \n",
            "图书馆员有\n"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# 设置不同的参数\n",
        "model_deterministic = init_chat_model(\"gpt-4o-mini\", temperature=0)  # 确定性输出\n",
        "model_creative = init_chat_model(\"gpt-4o-mini\", temperature=1)  # 创造性输出\n",
        "model_limited = init_chat_model(\"gpt-4o-mini\", temperature=0, max_tokens=50)  # 限制输出长度\n",
        "\n",
        "prompt = \"给我讲一个笑话\"\n",
        "\n",
        "print(\"Temperature=0:\")\n",
        "print(model_deterministic.invoke(prompt).content)\n",
        "\n",
        "print(\"\\nTemperature=1:\")\n",
        "print(model_creative.invoke(prompt).content)\n",
        "\n",
        "print(\"\\nMax tokens=50:\")\n",
        "print(model_limited.invoke(prompt).content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdjo2lCmwDWX"
      },
      "source": [
        "## 3. 灵活：无默认模型的可配置模型\n",
        "\n",
        "声明一个可配置模型，不指定默认模型，在运行时动态指定使用哪个模型。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXGPC-GtwDWX"
      },
      "source": [
        "### 3.1 声明完全可配置的模型\n",
        "\n",
        "不指定模型名称，创建一个可以在运行时配置的模型实例。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRMW63VlwDWX",
        "outputId": "06530fd9-5e7e-4a1d-8220-d8c6eef45d86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "可配置模型创建完成（无默认模型）\n"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# 不指定model参数，意味着必须在运行时通过config指定\n",
        "# configurable_fields会自动设置为\"any\"\n",
        "configurable_model = init_chat_model(temperature=0)\n",
        "\n",
        "print(\"可配置模型创建完成（无默认模型）\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJcizvKZwDWX"
      },
      "source": [
        "### 3.2 运行时指定不同模型\n",
        "\n",
        "通过 `config` 参数在每次调用时指定使用的模型。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qKtCQ55wDWX",
        "outputId": "713a77a0-a17a-48e7-8c52-30aa05615646"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "使用 GPT-4o:\n",
            "I’m called ChatGPT. How can I assist you today?\n"
          ]
        }
      ],
      "source": [
        "# 使用GPT-4o生成响应\n",
        "response_gpt = configurable_model.invoke(\n",
        "    \"what's your name\",\n",
        "    config={\"configurable\": {\"model\": \"gpt-4o\"}}\n",
        ")\n",
        "print(\"使用 GPT-4o:\")\n",
        "print(response_gpt.content)\n",
        "\n",
        "# 使用Claude Sonnet生成响应\n",
        "# response_claude = configurable_model.invoke(\n",
        "#     \"what's your name\",\n",
        "#     config={\"configurable\": {\"model\": \"claude-sonnet-4-5-20250929\"}}\n",
        "# )\n",
        "# print(\"\\n使用 Claude Sonnet:\")\n",
        "# print(response_claude.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZoYW2oZwDWX"
      },
      "source": [
        "### 3.3 运行时指定模型和参数\n",
        "\n",
        "除了模型，还可以在运行时指定其他参数。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFwVRv2NwDWX",
        "outputId": "86bf0137-6800-4418-ddf2-eac97cfd54a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "有一天，小明去买水果。他看到一个摊位上有很多苹果，便问老板：“这些苹果多少钱一个？”\n",
            "\n",
            "老板回答：“一个五块，三个十块。”\n",
            "\n",
            "小明想了想，问：“那我买两个，能不能给我打个折？”\n",
            "\n",
            "老板笑着说：“当然可以，你可以买一个五块，另一个也五块，这样总共十块，没打折！”\n",
            "\n",
            "小明无奈地说：“那我还是买三个吧，反正十块也是十块！”\n"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# 创建可配置模型，设置默认temperature\n",
        "configurable_model = init_chat_model(temperature=0)\n",
        "\n",
        "# 运行时同时指定模型和参数\n",
        "response = configurable_model.invoke(\n",
        "    \"讲一个笑话\",\n",
        "    config={\n",
        "        \"configurable\": {\n",
        "            \"model\": \"gpt-4o-mini\",\n",
        "            \"temperature\": 0.9,  # 覆盖默认的temperature=0\n",
        "            \"max_tokens\": 100\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zT-dRBA4wDWX"
      },
      "source": [
        "### 3.4 使用场景：动态模型选择\n",
        "\n",
        "适用于需要根据不同条件选择不同模型的场景。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTijcyDhwDWX",
        "outputId": "958f54bf-77aa-4910-bba4-d969c6a21ae8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "简单问题 (gpt-4o-mini):\n",
            "1 + 1 = 2.\n",
            "\n",
            "复杂问题 (gpt-4o):\n",
            "量子纠缠是量子力学中的一种现象，其中两个或多个粒子在量子状态上相互关联，以至于无论它们相距多远，对其中一个粒子的测量结果会立即影响另一个粒子的状态。这种现象最初由爱因斯坦、波多尔斯基和罗森在1935年 ...\n"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# 创建可配置模型\n",
        "flexible_model = init_chat_model(temperature=0)\n",
        "\n",
        "def ask_model(question: str, use_advanced: bool = False):\n",
        "    \"\"\"根据需求选择模型\"\"\"\n",
        "    model_name = \"gpt-4o\" if use_advanced else \"gpt-4o-mini\"\n",
        "\n",
        "    response = flexible_model.invoke(\n",
        "        question,\n",
        "        config={\"configurable\": {\"model\": model_name}}\n",
        "    )\n",
        "    return response.content\n",
        "\n",
        "# 简单问题用小模型\n",
        "print(\"简单问题 (gpt-4o-mini):\")\n",
        "print(ask_model(\"1+1=?\", use_advanced=False))\n",
        "\n",
        "# 复杂问题用大模型\n",
        "print(\"\\n复杂问题 (gpt-4o):\")\n",
        "print(ask_model(\"解释量子纠缠\", use_advanced=True)[:100], \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isArm2K7wDWX"
      },
      "source": [
        "## 4. 混合：有默认模型的可配置模型\n",
        "\n",
        "声明一个带默认模型的可配置模型，可以在运行时选择性地覆盖默认设置。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxTpBaGBwDWX"
      },
      "source": [
        "### 4.1 声明带默认值的可配置模型\n",
        "\n",
        "指定默认模型和参数，同时允许运行时覆盖。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAUVvue9wDWX",
        "outputId": "832736af-5c70-4130-ddac-d4024d24e90a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "带默认值的可配置模型创建完成\n",
            "默认: gpt-4o, temperature=0\n"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# 指定默认模型和参数，同时允许运行时配置\n",
        "configurable_model_with_default = init_chat_model(\n",
        "    \"openai:gpt-4o\",\n",
        "    configurable_fields=\"any\",  # 允许运行时配置所有字段\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "print(\"带默认值的可配置模型创建完成\")\n",
        "print(\"默认: gpt-4o, temperature=0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7CaPyB9wDWX"
      },
      "source": [
        "### 4.2 使用默认配置\n",
        "\n",
        "不传config参数时，使用初始化时的默认设置。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZ3CHcIqwDWX",
        "outputId": "84dd6bc1-9190-4b18-ea08-5a777f89f89a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "使用默认配置 (gpt-4o, temperature=0):\n",
            "I’m called ChatGPT. How can I assist you today?\n"
          ]
        }
      ],
      "source": [
        "# 使用默认配置：gpt-4o + temperature=0\n",
        "response = configurable_model_with_default.invoke(\"what's your name\")\n",
        "\n",
        "print(\"使用默认配置 (gpt-4o, temperature=0):\")\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_FMI63AwDWX"
      },
      "source": [
        "### 4.3 运行时覆盖配置\n",
        "\n",
        "传入config参数覆盖默认的模型和参数。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnNUtPiawDWX"
      },
      "outputs": [],
      "source": [
        "# 覆盖默认配置：使用Claude Sonnet + temperature=0.6\n",
        "# response = configurable_model_with_default.invoke(\n",
        "#     \"what's your name\",\n",
        "#     config={\n",
        "#         \"configurable\": {\n",
        "#             \"model\": \"anthropic:claude-sonnet-4-5-20250929\",\n",
        "#             \"temperature\": 0.6,\n",
        "#         }\n",
        "#     },\n",
        "# )\n",
        "\n",
        "# print(\"覆盖配置 (claude-sonnet, temperature=0.6):\")\n",
        "# print(response.content)\n",
        "\n",
        "print(\"演示代码（需要Anthropic API密钥）\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWMb2tOmwDWX"
      },
      "source": [
        "### 4.4 使用config_prefix管理配置\n",
        "\n",
        "使用配置前缀为多个模型创建独立的命名空间。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CH5cX63zwDWX"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# 使用config_prefix创建带命名空间的配置\n",
        "model_with_prefix = init_chat_model(\n",
        "    \"openai:gpt-4o\",\n",
        "    configurable_fields=\"any\",\n",
        "    config_prefix=\"foo\",  # 配置前缀\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "# 使用默认配置\n",
        "response1 = model_with_prefix.invoke(\"what's your name\")\n",
        "print(\"默认配置:\")\n",
        "print(response1.content)\n",
        "\n",
        "# 使用前缀覆盖配置\n",
        "# 注意：配置键需要加上前缀 \"foo_\"\n",
        "# response2 = model_with_prefix.invoke(\n",
        "#     \"what's your name\",\n",
        "#     config={\n",
        "#         \"configurable\": {\n",
        "#             \"foo_model\": \"anthropic:claude-sonnet-4-5-20250929\",  # 前缀_model\n",
        "#             \"foo_temperature\": 0.6,  # 前缀_temperature\n",
        "#         }\n",
        "#     },\n",
        "# )\n",
        "\n",
        "# print(\"\\n覆盖配置 (使用前缀):\")\n",
        "# print(response2.content)\n",
        "\n",
        "print(\"\\n配置前缀用于管理多个可配置模型\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kqnsod2nwDWX"
      },
      "source": [
        "### 4.5 多模型协作示例\n",
        "\n",
        "使用config_prefix管理多个模型的配置。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYehB8iHwDWX",
        "outputId": "fddc86e1-9898-401a-e612-59233492f04e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "总结: 人工智能正在通过在医疗、教育和交通等领域的广泛应用，深刻改变我们的世界。\n",
            "\n",
            "翻译: Artificial intelligence is profoundly transforming our world through its widespread applications in fields such as healthcare, education, and transportation.\n"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# 总结器模型\n",
        "summarizer = init_chat_model(\n",
        "    \"gpt-4o-mini\",\n",
        "    configurable_fields=\"any\",\n",
        "    config_prefix=\"summarizer\",\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "# 翻译器模型\n",
        "translator = init_chat_model(\n",
        "    \"gpt-4o-mini\",\n",
        "    configurable_fields=\"any\",\n",
        "    config_prefix=\"translator\",\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "# 统一配置两个模型\n",
        "unified_config = {\n",
        "    \"configurable\": {\n",
        "        \"summarizer_model\": \"gpt-4o-mini\",\n",
        "        \"summarizer_temperature\": 0,\n",
        "        \"translator_model\": \"gpt-4o\",\n",
        "        \"translator_temperature\": 0.3,\n",
        "    }\n",
        "}\n",
        "\n",
        "text = \"人工智能正在改变世界。它在医疗、教育、交通等领域都有广泛应用。\"\n",
        "\n",
        "# 总结\n",
        "summary = summarizer.invoke(\n",
        "    f\"用一句话总结：{text}\",\n",
        "    config=unified_config\n",
        ")\n",
        "print(\"总结:\", summary.content)\n",
        "\n",
        "# 翻译\n",
        "translation = translator.invoke(\n",
        "    f\"翻译成英文：{summary.content}\",\n",
        "    config=unified_config\n",
        ")\n",
        "print(\"\\n翻译:\", translation.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvPoXD5qwDWb"
      },
      "source": [
        "## 总结\n",
        "\n",
        "本notebook介绍了LangChain 1.0中模型声明和使用的核心模式：\n",
        "\n",
        "### 三种主要模式\n",
        "\n",
        "#### 1. 固定模型（Fixed Model）\n",
        "```python\n",
        "model = init_chat_model(\"openai:gpt-4o\", temperature=0)\n",
        "model.invoke(\"question\")\n",
        "```\n",
        "- **特点**: 模型和参数在初始化时固定\n",
        "- **优势**: 简单直接，配置明确\n",
        "- **适用**: 单一模型应用，生产环境\n",
        "\n",
        "#### 2. 无默认模型的可配置模型（Fully Configurable）\n",
        "```python\n",
        "model = init_chat_model(temperature=0)  # 不指定模型\n",
        "model.invoke(\"question\", config={\"configurable\": {\"model\": \"gpt-4o\"}})\n",
        "```\n",
        "- **特点**: 必须在运行时指定模型\n",
        "- **优势**: 最大灵活性，完全动态\n",
        "- **适用**: 多模型切换，A/B测试，用户选择模型\n",
        "\n",
        "#### 3. 有默认模型的可配置模型（Configurable with Default）\n",
        "```python\n",
        "model = init_chat_model(\"gpt-4o\", configurable_fields=\"any\", temperature=0)\n",
        "model.invoke(\"question\")  # 使用默认\n",
        "model.invoke(\"question\", config={\"configurable\": {\"model\": \"claude-sonnet\"}})  # 覆盖\n",
        "```\n",
        "- **特点**: 有默认值，可选择性覆盖\n",
        "- **优势**: 兼顾便利性和灵活性\n",
        "- **适用**: 大部分场景用默认，特殊情况切换\n",
        "\n",
        "### 核心参数\n",
        "\n",
        "| 参数 | 说明 | 示例 |\n",
        "|------|------|------|\n",
        "| `model` | 模型标识符 | `\"openai:gpt-4o\"`, `\"claude-sonnet\"` |\n",
        "| `temperature` | 随机性控制 (0-1) | `0` (确定), `1` (创造) |\n",
        "| `configurable_fields` | 可配置字段 | `\"any\"`, `(\"model\", \"temperature\")` |\n",
        "| `config_prefix` | 配置前缀 | `\"foo\"` → 配置键为 `\"foo_model\"` |\n",
        "| `max_tokens` | 最大输出长度 | `100`, `1000` |\n",
        "\n",
        "### 高级特性\n",
        "\n",
        "- **工具绑定**: 使用 `.bind_tools()` 为模型添加函数调用能力\n",
        "- **配置前缀**: 使用 `config_prefix` 管理多个可配置模型\n",
        "- **运行时覆盖**: 通过 `config` 参数灵活调整任何参数\n",
        "\n",
        "### 最佳实践\n",
        "\n",
        "1. **开发阶段**: 使用可配置模型快速实验不同模型\n",
        "2. **生产环境**: 根据稳定性需求选择固定或可配置模式\n",
        "3. **多模型应用**: 使用 `config_prefix` 管理多个模型的配置\n",
        "4. **成本优化**: 默认使用小模型，复杂任务时切换大模型\n",
        "5. **工具调用**: 选择支持工具调用的模型（GPT-4o, Claude等）\n",
        "\n",
        "LangChain 1.0的设计理念：**统一接口 + 灵活配置**，让你可以轻松切换15+个提供商的模型，同时保持代码简洁。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
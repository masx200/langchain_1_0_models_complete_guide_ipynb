# LangChain 1.0 æ¨¡å‹å£°æ˜ä¸ä½¿ç”¨å®Œå…¨æŒ‡å—

æœ¬é¡¹ç›®æä¾›äº†LangChain 1.0ä¸­æ¨¡å‹å£°æ˜å’Œä½¿ç”¨çš„å…¨é¢æŒ‡å—ï¼Œè¯¦ç»†ä»‹ç»äº†ä¸‰ç§ä¸»è¦çš„æ¨¡å‹é…ç½®æ¨¡å¼åŠå…¶çµæ´»åº”ç”¨ã€‚

## ğŸ“‹ ç›®å½•

- [æ¦‚è¿°](#æ¦‚è¿°)
- [åŠŸèƒ½ç‰¹æ€§](#åŠŸèƒ½ç‰¹æ€§)
- [å®‰è£…è¦æ±‚](#å®‰è£…è¦æ±‚)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [ä¸‰ç§ä¸»è¦æ¨¡å¼](#ä¸‰ç§ä¸»è¦æ¨¡å¼)
- [ä½¿ç”¨åœºæ™¯](#ä½¿ç”¨åœºæ™¯)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
- [è®¸å¯è¯](#è®¸å¯è¯)

## æ¦‚è¿°

LangChain 1.0å¼•å…¥äº†ä¸€ç§ç»Ÿä¸€çš„æ¨¡å‹æ¥å£è®¾è®¡ï¼Œæ”¯æŒ15+ä¸ªAIæ¨¡å‹æä¾›å•†ï¼ˆOpenAIã€Anthropicã€Googleç­‰ï¼‰ã€‚æœ¬æŒ‡å—æ·±å…¥è®²è§£äº†æ¨¡å‹çš„å£°æ˜æ–¹å¼ã€é…ç½®å‚æ•°å’Œå®é™…åº”ç”¨ï¼Œå¸®åŠ©å¼€å‘è€…çµæ´»é«˜æ•ˆåœ°ä½¿ç”¨å„ç§å¤§è¯­è¨€æ¨¡å‹ã€‚

## åŠŸèƒ½ç‰¹æ€§

- ğŸš€ **ç»Ÿä¸€æ¥å£**ï¼šä¸€å¥—ä»£ç æ”¯æŒå¤šä¸ªæ¨¡å‹æä¾›å•†
- ğŸ”§ **çµæ´»é…ç½®**ï¼šæ”¯æŒè¿è¡Œæ—¶åŠ¨æ€åˆ‡æ¢æ¨¡å‹å’Œå‚æ•°
- ğŸ“š **ä¸‰ç§æ¨¡å¼**ï¼šå›ºå®šæ¨¡å‹ã€å®Œå…¨å¯é…ç½®ã€å¯é…ç½®+é»˜è®¤å€¼
- ğŸ¯ **æ™ºèƒ½æ¨æ–­**ï¼šè‡ªåŠ¨è¯†åˆ«æ¨¡å‹æä¾›å•†
- ğŸ› ï¸ **å·¥å…·ç»‘å®š**ï¼šæ”¯æŒå‡½æ•°è°ƒç”¨å’Œå·¥å…·é›†æˆ
- ğŸ“Š **å‚æ•°æ§åˆ¶**ï¼štemperatureã€max_tokensç­‰å®Œæ•´å‚æ•°æ”¯æŒ

## å®‰è£…è¦æ±‚

### Pythonç‰ˆæœ¬
Python 3.8+

### ä¾èµ–åŒ…
```bash
pip install langchain langchain-openai langchain-anthropic langchain-google-vertexai
```

### APIå¯†é’¥é…ç½®
æ ¹æ®ä½¿ç”¨çš„æ¨¡å‹æä¾›å•†é…ç½®ç›¸åº”çš„APIå¯†é’¥ï¼š

```python
import os
from google.colab import userdata

# OpenAI
os.environ["OPENAI_API_KEY"] = userdata.get('OPENAI_API_KEY')

# Anthropic (å¯é€‰)
# os.environ["ANTHROPIC_API_KEY"] = "your-anthropic-api-key"

# Google Vertex AI (å¯é€‰)
# os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "path/to/credentials.json"
```

## å¿«é€Ÿå¼€å§‹

### æœ€ç®€å•çš„ä½¿ç”¨æ–¹å¼
```python
from langchain.chat_models import init_chat_model

# å£°æ˜å¹¶ä½¿ç”¨æ¨¡å‹
model = init_chat_model("openai:gpt-4o", temperature=0)
response = model.invoke("ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±")
print(response.content)
```

### è‡ªåŠ¨æ¨æ–­æä¾›å•†
```python
# ä¸éœ€è¦æ˜¾å¼æŒ‡å®šæä¾›å•†ï¼Œç³»ç»Ÿè‡ªåŠ¨è¯†åˆ«
model = init_chat_model("gpt-4o-mini", temperature=0)
response = model.invoke("è®²ä¸€ä¸ªç¬‘è¯")
```

## ä¸‰ç§ä¸»è¦æ¨¡å¼

### 1. å›ºå®šæ¨¡å‹ (Fixed Model)
**ç‰¹ç‚¹**ï¼šæ¨¡å‹å’Œå‚æ•°åœ¨åˆå§‹åŒ–æ—¶å›ºå®š

```python
# æœ€ç›´æ¥çš„ä½¿ç”¨æ–¹å¼
model = init_chat_model("openai:gpt-4o", temperature=0, max_tokens=100)
response = model.invoke("what's your name")
```

**é€‚ç”¨åœºæ™¯**ï¼š
- å•ä¸€æ¨¡å‹åº”ç”¨
- ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
- æ˜ç¡®çš„æ¨¡å‹éœ€æ±‚

### 2. å®Œå…¨å¯é…ç½®æ¨¡å‹ (Fully Configurable)
**ç‰¹ç‚¹**ï¼šä¸æŒ‡å®šé»˜è®¤æ¨¡å‹ï¼Œè¿è¡Œæ—¶åŠ¨æ€æŒ‡å®š

```python
# åˆ›å»ºå¯é…ç½®æ¨¡å‹
configurable_model = init_chat_model(temperature=0)

# è¿è¡Œæ—¶æŒ‡å®šæ¨¡å‹
response = configurable_model.invoke(
    "what's your name",
    config={"configurable": {"model": "gpt-4o"}}
)
```

**é€‚ç”¨åœºæ™¯**ï¼š
- å¤šæ¨¡å‹åˆ‡æ¢
- A/Bæµ‹è¯•
- ç”¨æˆ·é€‰æ‹©æ¨¡å‹
- æˆæœ¬ä¼˜åŒ–ç­–ç•¥

### 3. å¯é…ç½®+é»˜è®¤å€¼æ¨¡å‹ (Configurable with Default)
**ç‰¹ç‚¹**ï¼šæœ‰é»˜è®¤å€¼ï¼Œå¯é€‰æ‹©æ€§è¦†ç›–

```python
# å¸¦é»˜è®¤å€¼çš„å¯é…ç½®æ¨¡å‹
model = init_chat_model(
    "openai:gpt-4o",
    configurable_fields="any",
    temperature=0
)

# ä½¿ç”¨é»˜è®¤é…ç½®
response1 = model.invoke("what's your name")

# è¿è¡Œæ—¶è¦†ç›–é…ç½®
response2 = model.invoke(
    "what's your name",
    config={"configurable": {"model": "claude-sonnet", "temperature": 0.6}}
)
```

**é€‚ç”¨åœºæ™¯**ï¼š
- å¤§éƒ¨åˆ†åœºæ™¯ç”¨é»˜è®¤é…ç½®
- ç‰¹æ®Šæƒ…å†µéœ€è¦åˆ‡æ¢æ¨¡å‹
- éœ€è¦çµæ´»æ€§çš„ç”Ÿäº§ç¯å¢ƒ

## é«˜çº§ç‰¹æ€§

### é…ç½®å‰ç¼€ (config_prefix)
ä½¿ç”¨å‘½åç©ºé—´ç®¡ç†å¤šä¸ªå¯é…ç½®æ¨¡å‹ï¼š

```python
# æ€»ç»“å™¨æ¨¡å‹
summarizer = init_chat_model(
    "gpt-4o-mini",
    configurable_fields="any",
    config_prefix="summarizer",
    temperature=0
)

# ç¿»è¯‘å™¨æ¨¡å‹
translator = init_chat_model(
    "gpt-4o-mini",
    configurable_fields="any",
    config_prefix="translator",
    temperature=0
)

# ç»Ÿä¸€é…ç½®
unified_config = {
    "configurable": {
        "summarizer_model": "gpt-4o-mini",
        "translator_model": "gpt-4o",
    }
}

summary = summarizer.invoke("æ€»ç»“è¿™æ®µæ–‡æœ¬", config=unified_config)
translation = translator.invoke("ç¿»è¯‘è¿™æ®µæ–‡æœ¬", config=unified_config)
```

### è¿è¡Œæ—¶å‚æ•°è¦†ç›–
```python
# è¿è¡Œæ—¶åŒæ—¶æŒ‡å®šæ¨¡å‹å’Œå‚æ•°
response = configurable_model.invoke(
    "è®²ä¸€ä¸ªç¬‘è¯",
    config={
        "configurable": {
            "model": "gpt-4o-mini",
            "temperature": 0.9,
            "max_tokens": 100
        }
    }
)
```

### æ™ºèƒ½æ¨¡å‹é€‰æ‹©
```python
def ask_model(question: str, use_advanced: bool = False):
    """æ ¹æ®éœ€æ±‚é€‰æ‹©æ¨¡å‹"""
    model_name = "gpt-4o" if use_advanced else "gpt-4o-mini"
    
    flexible_model = init_chat_model(temperature=0)
    
    response = flexible_model.invoke(
        question,
        config={"configurable": {"model": model_name}}
    )
    return response.content

# ç®€å•é—®é¢˜ç”¨å°æ¨¡å‹
simple_answer = ask_model("1+1=?", use_advanced=False)

# å¤æ‚é—®é¢˜ç”¨å¤§æ¨¡å‹
complex_answer = ask_model("è§£é‡Šé‡å­çº ç¼ ", use_advanced=True)
```

## ä½¿ç”¨åœºæ™¯

### 1. æˆæœ¬ä¼˜åŒ–ç­–ç•¥
```python
# é»˜è®¤ä½¿ç”¨ä¾¿å®œçš„å°æ¨¡å‹
default_model = init_chat_model(
    "gpt-4o-mini",
    configurable_fields="any",
    temperature=0
)

# å¤æ‚ä»»åŠ¡æ—¶åˆ‡æ¢åˆ°æ›´å¼ºæ¨¡å‹
def process_request(question, complexity="simple"):
    model = "gpt-4o" if complexity == "complex" else "gpt-4o-mini"
    
    return default_model.invoke(
        question,
        config={"configurable": {"model": model}}
    )
```

### 2. A/Bæµ‹è¯•
```python
# åˆ›å»ºä¸¤ä¸ªé…ç½®çš„æ¨¡å‹ç”¨äºA/Bæµ‹è¯•
model_a = init_chat_model("gpt-4o-mini", temperature=0.7)
model_b = init_chat_model("gpt-4o-mini", temperature=0.9)

# åˆ†é…ç”¨æˆ·åˆ°ä¸åŒç»„è¿›è¡Œæµ‹è¯•
def ab_test_response(question, user_group):
    model = model_a if user_group == "A" else model_b
    return model.invoke(question)
```

### 3. å¤šåŠŸèƒ½åº”ç”¨ç³»ç»Ÿ
```python
# åˆ›å»ºä¸€ä¸ªç»Ÿä¸€çš„å¤šåŠŸèƒ½æ¨¡å‹æ¥å£
multi_model = init_chat_model(
    "gpt-4o-mini",
    configurable_fields="any",
    temperature=0
)

def chat_system(question, mode="general"):
    configs = {
        "coding": {"model": "gpt-4o", "temperature": 0.2},
        "creative": {"model": "gpt-4o", "temperature": 0.8},
        "general": {"model": "gpt-4o-mini", "temperature": 0.5}
    }
    
    return multi_model.invoke(
        question,
        config={"configurable": configs[mode]}
    )
```

## æœ€ä½³å®è·µ

### 1. å¼€å‘é˜¶æ®µ
- ä½¿ç”¨å¯é…ç½®æ¨¡å‹å¿«é€Ÿå®éªŒä¸åŒæ¨¡å‹
- åˆ©ç”¨ `temperature` å‚æ•°æµ‹è¯•ä¸åŒè¾“å‡ºé£æ ¼
- ä½¿ç”¨å°æ¨¡å‹è¿›è¡Œåˆæ­¥æµ‹è¯•ï¼ŒèŠ‚çœæˆæœ¬

### 2. ç”Ÿäº§ç¯å¢ƒ
- **ç¨³å®šæ€§ä¼˜å…ˆ**ï¼šä½¿ç”¨å›ºå®šæ¨¡å‹æˆ–è®¾ç½®æ˜ç¡®çš„é»˜è®¤æ¨¡å‹
- **ç›‘æ§åˆ‡æ¢**ï¼šè®°å½•æ¨¡å‹è°ƒç”¨æƒ…å†µï¼Œä¾¿äºè¿½è¸ªé—®é¢˜
- **é”™è¯¯å¤„ç†**ï¼šä¸ºæ¨¡å‹åˆ‡æ¢è®¾ç½®fallbackæœºåˆ¶

### 3. å¤šæ¨¡å‹åº”ç”¨
- ä½¿ç”¨ `config_prefix` ç®¡ç†å¤šä¸ªæ¨¡å‹çš„é…ç½®
- ç»Ÿä¸€é…ç½®ç®¡ç†ï¼Œé¿å…é…ç½®åˆ†æ•£
- å»ºç«‹æ¨¡å‹é€‰æ‹©çš„ç­–ç•¥å’Œè§„åˆ™

### 4. æˆæœ¬ä¼˜åŒ–
- é»˜è®¤ä½¿ç”¨æ€§èƒ½-æˆæœ¬æ¯”é«˜çš„å°æ¨¡å‹
- å¤æ‚ä»»åŠ¡æ—¶åˆ‡æ¢åˆ°å¤§æ¨¡å‹
- è®¾ç½®åˆç†çš„ `max_tokens` é™åˆ¶è¾“å‡ºé•¿åº¦
- è€ƒè™‘ä½¿ç”¨æ‰¹å¤„ç†å‡å°‘APIè°ƒç”¨æ¬¡æ•°

### 5. å·¥å…·è°ƒç”¨
- é€‰æ‹©æ”¯æŒå·¥å…·è°ƒç”¨çš„æ¨¡å‹ï¼ˆGPT-4o, Claudeç­‰ï¼‰
- ä½¿ç”¨ `.bind_tools()` ä¸ºæ¨¡å‹æ·»åŠ å‡½æ•°è°ƒç”¨èƒ½åŠ›
- åˆç†è®¾è®¡å·¥å…·æ¥å£ï¼Œå‡å°‘è°ƒç”¨å¤æ‚åº¦

### 6. å‚æ•°é…ç½®æŒ‡å—

| å‚æ•° | æ¨èå€¼ | åœºæ™¯ |
|------|--------|------|
| temperature | 0 | ç¡®å®šæ€§è¾“å‡ºï¼Œä»£ç ç”Ÿæˆï¼Œäº‹å®é—®ç­” |
| temperature | 0.3-0.5 | ä¸€èˆ¬å¯¹è¯ï¼Œåˆ›æ„ä»»åŠ¡ |
| temperature | 0.8-1.0 | åˆ›æ„å†™ä½œï¼Œå¤´è„‘é£æš´ |
| max_tokens | 100 | ç®€çŸ­å›ç­” |
| max_tokens | 500 | è¯¦ç»†è§£é‡Š |
| max_tokens | 1000+ | é•¿æ–‡æœ¬ç”Ÿæˆ |

## æ”¯æŒçš„æ¨¡å‹æä¾›å•†

- **OpenAI**: GPT-4, GPT-4o, GPT-3.5
- **Anthropic**: Claude Sonnet, Claude Haiku
- **Google**: Gemini, Vertex AI
- **å…¶ä»–**: æ”¯æŒ15+ä¸»æµæ¨¡å‹æä¾›å•†

## è®¸å¯è¯

æœ¬é¡¹ç›®éµå¾ª MIT è®¸å¯è¯ã€‚è¯¦æƒ…è¯·æŸ¥çœ‹ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

## è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestæ¥æ”¹è¿›è¿™ä¸ªæŒ‡å—ï¼

## ä½œè€…

**MiniMax Agent** - LangChain 1.0 æ¨¡å‹ä½¿ç”¨æŒ‡å—

---

*æœ¬æŒ‡å—åŸºäºLangChain 1.0ç‰ˆæœ¬ç¼–å†™ï¼Œå»ºè®®å®šæœŸæ›´æ–°ä»¥è·Ÿéšæœ€æ–°ç‰ˆæœ¬ã€‚*